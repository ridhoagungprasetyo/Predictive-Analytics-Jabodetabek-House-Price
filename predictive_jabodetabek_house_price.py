# -*- coding: utf-8 -*-
"""Predictive jabodetabek house price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RZz1rsXenKQUjQZlDL3oAd4YHfFcwCAB

# 1. Business Understanding

Dengan harga rumah yang meningkat rata-rata 8% per tahun, lebih tinggi dari rata-rata nasional yang hanya sekitar 6%, calon pembeli dan investor sering kali menghadapi kesulitan dalam menentukan nilai wajar dari properti. Ketidakpastian ini dapat mengakibatkan keputusan investasi yang tidak tepat, yang pada gilirannya dapat mempengaruhi stabilitas pasar dan aksesibilitas perumahan bagi masyarakat.

Saat ini, banyak informasi mengenai harga rumah yang tidak tersedia secara terbuka, sehingga calon pembeli dan investor kesulitan dalam membuat keputusan yang informasi. Dengan menyediakan data yang dapat diandalkan dan analisis yang mendalam tentang faktor-faktor yang memengaruhi harga rumah, proyek ini diharapkan dapat membantu menciptakan lingkungan pasar yang lebih transparan. Hal ini tidak hanya akan memberikan kepercayaan kepada pembeli tetapi juga mendorong pengembang untuk beroperasi dengan lebih etis dan bertanggung jawab.

# 2. Importing Library
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tabulate import tabulate
# %matplotlib inline
import seaborn as sns

"""# 3. Data Loading"""

data = pd.read_csv('jabodetabek_house_price.csv')
data

"""Dari data di atas, terlihat bahwa terdapat 27 Fitur dengan jumlah pengamatan sebanyak 3553 baris

# 4. Data Understanding

## A. Jumlah Masing-masing Variabel Beserta Tipenya
"""

data.info()

"""Dari data diatas terlihat bahwa terdiri dari 14 data bertipe float64, 13 data bertipe object.
Dataset berisi 3553 records dan 27 kolom yang memiliki karakteristik sebagai berikut:
- url : Link sumber data
- price_in_rp : Harga properti dalam satuan Rp (rupiah).
- title : Judul atau nama properti.
- address : Alamat lengkap properti.
- district : Wilayah administratif yang lebih kecil daripada kabupaten/kota, namun masih berstatus wilayah administratif.
- city : Nama kota dimana properti berada.
- lat : Koordinat lintang (latitude), nilai antara -90° sampai +90°.
- long : Koordinat bujur (longitude), nilai antara -180° sampai +180°.
- facilities : Fasilitas yang disediakan bersamaan dengan sewaan/milik properti.
- property_type : Jenis properti (rumah, apartemen, villa, dll.)
- ads_id : ID iklan properti.
- bedrooms : Jumlah kamar tidur.
- bathrooms : Jumlah kamar mandi.
- land_size_m2 : Luas tanah milik properti dalam meter persegi.
- building_size_m2 : Luas bangunan properti dalam meter persegi.
- carports : Tempat penyimpanan kendaraan mobil
- certificate : Dokumen legal yang menunjukkan kepemilikan atau hak atas suatu properti
- electricity : Sistem listrik yang digunakan di properti
- maid_bedrooms : Jumlah kamar tidur untuk pelayan.
- maid_bathrooms : Jumlah kamar mandi untuk pelayan.
- floors : Tingkat/titik tertinggi dari sebuah struktur bangunan.
- building_age : Umur bangunan
- year_built : Tahun pembangunan properti
- property_condition : Status kondisi properti sekarang ini (baru,sedang renovasi,dll.)
- building_orientation : Orientasi arah bangunan relatif matahari/sinar matahari
- garages : Lahan parkir kendaraan
- furnishing : Pengaturan interior/dekorasi ruangan

## B. Deskripsi Variabel

## C. Deskripsi Statistik Data
"""

# melihat deskripsi statistik
data.describe()

"""Fungsi describe() memberikan informasi statistik pada masing - masing kolom, antara lain:
- Count  adalah jumlah sampel pada data.
- Mean adalah nilai rata-rata.
- Std adalah standar deviasi.
- Min yaitu nilai minimum setiap kolom.
- 25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama.
- 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
- 75% adalah kuartil ketiga.
- Max adalah nilai maksimum.

# 5. Data Cleaning

## A. Menangani Missing Value

Memeriksa informasi apakah ada nilai NaN/Null
"""

def check_nan(data):
    nan_counts = data.isna().sum()
    columns_with_nan = nan_counts[nan_counts > 0].index.tolist()

    return pd.DataFrame({
        'Fitur': columns_with_nan,
        'Jumlah NaN/Null': nan_counts[columns_with_nan].tolist()
    })

print("Informasi NaN/Null dalam bentuk DataFrame:")
print(check_nan(data))

"""Terlihat ada banyak nilai NaN yang ada pada setiap Fitur"""

# hapus fitur yang mengandung NaN
data_cleaned = data.dropna(axis=1, how='any')

print("Fitur yang tersisa setelah menghapus fitur dengan NaN:")
print(data_cleaned.columns.tolist())

"""Fitur yang terdaapat Nilai NaN sudah terhapus sehingga tidak lagi muncul dalam data terbaru sehingga hanya tersisa 14 Fitur."""

# Memeriksa ukuran data yang telah dihapus beberapa barisnya
data_cleaned.shape

"""Jumlah data setelah menghapus beberapa Fitur menunjukkan terdapat 3553 baris dan 14 kolom Fitur yang tersisa

## B. Memeriksa Outlier
"""

# periksa outlier pada kolom numerik

import matplotlib.pyplot as plt

# Pilih kolom numerik yang ingin diperiksa
numerical_cols = data_cleaned.select_dtypes(include=np.number).columns

# Buat boxplot untuk setiap kolom numerik
for col in numerical_cols:
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=data_cleaned[col])
  plt.title(f"Boxplot for {col}")
  plt.show()

"""Terlihat bahwa terdapat beberapa outlier pada kolom-kolom di atas. Pada kasus ini, kita akan menghapus outlier menggunakan teknik Inter Quartile Range (IQR). IQR didefinisikan sebagai
* $IQR = Q3 - Q1$
* Batas Bawah = $Q1 - 1.5 * IQR$
* Batas Atas = $Q3 + 1.5 * IQR$
"""

numeric_columns = data_cleaned.select_dtypes(include = ["number"]).columns
# Calculate Q1, Q3, and IQR only for numeric columns
Q1 = data_cleaned[numeric_columns].quantile(0.25)
Q3 = data_cleaned[numeric_columns].quantile(0.75)
IQR = Q3 - Q1
# Filter the DataFrame based on the IQR
data_cleaned = data_cleaned[~((data_cleaned[numeric_columns] < (Q1 - 1.5 * IQR)) |
                    (data_cleaned[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis = 1)]
data_cleaned.shape

"""Setelah outlier dihapus data tersisa 3076 baris dan 14 coulomns"""

# Pilih kolom numerik yang ingin diperiksa
numerical_cols = data_cleaned.select_dtypes(include=np.number).columns

# Buat boxplot untuk setiap kolom numerik
for col in numerical_cols:
  plt.figure(figsize=(8, 6))
  sns.boxplot(x=data_cleaned[col])
  plt.title(f"Boxplot for {col}")
  plt.show()

"""Bisa Dilihat bahwa outlier yang ada sudah terhapus dengan metode IQR

## C. Memeriksa Data Duplikat
"""

# Menampilkan jumlah data duplikat
data_cleaned.duplicated().sum()

"""Berdasarkan hasil di atas, tidak terdapat data duplikat pada data"""

data_cleaned.info()

"""Setelah melalui beberapa tahapan, data berjumlah 3076 baris dan 14 Fitur

# 6. Exploratory Data Analysis

## A. Univariate Analysis
"""

# Memisahkan kolom numerik dan kategorikal
numerical_cols = data_cleaned.select_dtypes(include=np.number).columns.tolist()
categorical_cols = data_cleaned.select_dtypes(include=['object']).columns.tolist()

print("Kolom Numerik:")
print(numerical_cols)
print("\nKolom Kategorikal:")
print(categorical_cols)

"""Bisa dilihat mengenai hasil pembagian fitur berdasarkan tipe data dari setiap Fitur"""

# Membagi kolom-kolom menjadi kolom numerikal dan kolom kategorikal
numerical_features = ['price_in_rp', 'lat', 'long', 'carports', 'maid_bedrooms', 'maid_bathrooms', 'garages']
categorical_features = ['url', 'title', 'address', 'district', 'city', 'facilities', 'electricity']

# Analisis Fitur district
feature = categorical_features[3]
count = data_cleaned[feature].value_counts()
percent = 100*data_cleaned[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(df)

"""Hasil analisis fitur district menunjukkan jumlah sampel dan presentase dari data wilayah administratif

Analisis Fitur Numerik
"""

# Numerical feature
data_cleaned.hist(bins = 50, figsize = (20, 15))
plt.show()

"""Dilihat dari histogram tersebut kita mendapatkan keterangan bahwa:

- price_in_rp: Histogram ini menunjukkan distribusi harga rumah dalam Rupiah. Sebagian besar harga rumah terkonsentrasi pada kisaran lebih rendah (sekitar di bawah 1 miliar), dengan semakin sedikit rumah yang memiliki harga di atas 2 miliar. Ini menunjukkan distribusi yang sangat condong ke kiri.

- lat (latitude): Fitur ini menunjukkan distribusi letak geografis rumah berdasarkan garis lintang. Distribusi latitude cukup merata, namun terdapat puncak pada beberapa nilai tertentu, yang mungkin menunjukkan lokasi rumah di area tertentu yang lebih padat.

- long (longitude): Fitur ini menunjukkan distribusi letak geografis rumah berdasarkan garis bujur. Histogramnya memperlihatkan variasi yang cukup merata, tetapi terdapat beberapa puncak pada titik tertentu, yang mungkin menunjukkan area yang lebih populer.

- carports: Fitur ini menunjukkan jumlah carport di setiap rumah. Mayoritas rumah memiliki 1 carport, diikuti oleh 0 dan 2 carport. Sangat sedikit rumah yang memiliki lebih dari 2 carport.

- maid_bedrooms: Histogram ini menunjukkan jumlah kamar tidur untuk pembantu di rumah. Sebagian besar rumah tidak memiliki kamar tidur untuk pembantu, dan beberapa memiliki 1 kamar tidur pembantu.

- maid_bathrooms: Fitur ini menunjukkan jumlah kamar mandi pembantu. Mayoritas rumah tidak memiliki kamar mandi khusus untuk pembantu, dan sebagian kecil memiliki 1 kamar mandi pembantu.

- garages: Histogram ini menunjukkan jumlah garasi di setiap rumah. Mayoritas rumah tidak memiliki garasi, dan sebagian kecil memiliki 1 atau 2 garasi.

Secara keseluruhan, hasil histogram ini memberikan gambaran distribusi setiap fitur numerik. Sebagian besar fitur menunjukkan distribusi yang sangat tidak merata atau terpusat pada nilai tertentu, yang bisa menjadi informasi penting dalam analisis dan prediksi harga rumah.

## B. Multivariate Analysis
"""

print(data_cleaned['price_in_rp'])

"""Bisa dilihat values dari kolom price_in_rp"""

# Menghitung rata-rata price_in_rp untuk setiap city
average_price_by_city = data_cleaned.groupby('city')['price_in_rp'].mean()

# Convert the Series to a list of lists for tabulate
average_price_by_city_list = [[city, "Rp {:,.0f} ".format(price)] for city, price in average_price_by_city.items()]

# Menampilkan hasil
print(tabulate(average_price_by_city_list, headers=['City', 'Average Price'], tablefmt='psql'))

# Membuat plot untuk memvisualisasikan pengaruh city terhadap harga
plt.figure(figsize=(10, 6))
sns.barplot(x=average_price_by_city.index, y=average_price_by_city.values)
plt.title('Pengaruh City terhadap Harga')
plt.xticks(rotation=45)
plt.show()

"""Jika dilihat barplot tersebut bisa kita simpulkan bahwa kota sangat mempengaruhi harga jual dari rummah dan perbedaan setiap harga rumah disetiap kota juga beberapa sangat signifikan"""

# Heatmap untuk visualisasi korelasi
plt.figure(figsize=(12, 8))

# Select only numeric columns for correlation calculation
numerical_data = data_cleaned.select_dtypes(include=['number'])

correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Korelasi Antar Fitur Numerik')
plt.show()

"""Jika kita amati, fitur 'garages' memiliki skor korelasi yang kecil (0.16) dengan fitur target ‘price_in_rp’. Sehingga, fitur tersebut dapat di-drop.

# 7. Data Preparation
"""

data_cleaned.head()

data_cleaned.drop(['garages'], inplace=True, axis=1)
data_cleaned.head()

"""Dari hasil pembacaan 5 bari awal dari data kita sudah tidak ada kolom 'garages'"""

# Encoding Fitur Kategori
cat_features = data_cleaned.select_dtypes(include='object').columns.to_list()
for feature in cat_features:
    dummies = pd.get_dummies(data_cleaned[feature], prefix=feature)
    data_cleaned = pd.concat([data_cleaned, dummies], axis=1)
data_cleaned.drop(cat_features, axis=1, inplace=True)
data_cleaned.head()

"""Proses encoding fitur kategori menggunakan teknik one-hot-encoding. Teknik ini adalah salah satu metode dalam proses encoding fitur (feature encoding) pada data kategorikal. Tujuannya adalah untuk mengubah variabel kategorikal menjadi representasi biner yang dapat digunakan dalam algoritma pembelajaran mesin. Kita memiliki beberapa variabel kategori. Mari kita lakukan proses encoding ini dengan fitur get_dummies."""

numerical_features = numerical_features = data_cleaned.select_dtypes(include=['float64', 'int64'])
sns.pairplot(data_cleaned[numerical_features.columns.tolist()], plot_kws={"s": 3})

"""Teknik reduksi (pengurangan) dimensi adalah prosedur yang mengurangi jumlah fitur dengan tetap mempertahankan informasi pada data. Teknik pengurangan dimensi yang digunakan pada proyek ini adalah PCA. PCA adalah teknik untuk mereduksi dimensi, mengekstraksi fitur, dan mentransformasi data dari “n-dimensional space” ke dalam sistem berkoordinat baru dengan dimensi m, di mana m lebih kecil dari n.

PCA bekerja menggunakan metode aljabar linier. Ia mengasumsikan bahwa sekumpulan data pada arah dengan varians terbesar merupakan yang paling penting (utama). PCA umumnya digunakan ketika variabel dalam data memiliki korelasi yang tinggi. Korelasi tinggi ini menunjukkan data yang berulang atau redundant. Karena hal inilah, teknik PCA digunakan untuk mereduksi variabel asli menjadi sejumlah kecil variabel baru yang tidak berkorelasi linier, disebut komponen utama (PC). Komponen utama ini dapat menangkap sebagian besar varians dalam variabel asli. Sehingga, saat teknik PCA diterapkan pada data, ia hanya akan menggunakan komponen utama dan mengabaikan sisanya.
"""

sns.pairplot(data_cleaned[['lat', 'long']], plot_kws={"s": 3})

"""Berdasarkan dataframe fitur numerik dan pairplot, terdapat beberapa fitur yang akan dilakukan proses reduksi. Fitur lat dan long memiliki korelasi yang cukup tinggi. Hal ini terjadi karena fitur tersebut mengandung informasi yang sama yaitu koordinat lokasi."""

from sklearn.decomposition import PCA

pca = PCA(n_components=2, random_state=123)
pca.fit(data_cleaned[['lat','long']])
princ_comp = pca.transform(data_cleaned[['lat','long']])

"""Kode di atas memanggil class PCA() dari library scikit-learn. Paremeter yang kita masukkan ke dalam class adalah n_components dan random_state. Parameter n_components merupakan jumlah komponen atau dimensi, dalam kasus kita jumlahnya ada 2, yaitu 'lat' dan 'long'.

Sedangkan, parameter random_state berfungsi untuk mengontrol random number generator yang digunakan. Parameter ini berupa bilangan integer dan nilainya bebas. Pada kasus ini, kita menerapkan random_state = 123. Berapa pun nilai integer yang kita tentukan (selama itu bilangan integer), ia akan memberikan hasil yang sama setiap kali dilakukan pemanggilan fungsi (dalam kasus kita, class PCA).

Setelah class PCA dibuat, kita bisa mengetahui proporsi informasi dari kedua komponen tersebut.
"""

pca.explained_variance_ratio_.round(3)

"""Arti dari output diatas adalah, 61,6% informasi pada kedua fitur 'GrLivArea' dan 'GarageArea' terdapat pada Principal Component (PC) pertama. Sedangkan sisanya, sebesar 3,84% terdapat pada PC kedua. Berdasarkan hasil ini, kita akan mereduksi fitur (dimensi) dan hanya mempertahankan PC (komponen) pertama saja. PC pertama ini akan menjadi fitur dimensi atau ukuran area menggantikan fitur 'lat' dan 'long'. Kita beri nama fitur ini 'dimension'."""

pca = PCA(n_components=1, random_state=123)
pca.fit(data_cleaned[['lat','long']])
data_cleaned['dimension'] = pca.fit_transform(data_cleaned.loc[:, ('lat','long')]).flatten()
data_cleaned.drop(['lat','long'], axis=1, inplace=True)

data_cleaned

"""### Train-Test Split

Selanjutnya adalah membagi dataser menjadi data latih (train) dan data uji (test). Proses pembagian dataset menggunakan library sklearn yaitu train-test-split. Proporsi pembagian adalah 80:20. Tidak lupa juga kita akan memisahkan fitur dengan target (label) yaitu SalePrice.
"""

from sklearn.model_selection import train_test_split

X = data_cleaned.drop(["price_in_rp"],axis =1)
y = data_cleaned["price_in_rp"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

"""Untuk mengecek jumlah sampel pada masing-masing bagian, kita gunakan code berikut."""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### Standarisasi

Standardisasi adalah teknik transformasi yang paling umum digunakan dalam tahap persiapan pemodelan. Untuk fitur numerik, kita tidak akan melakukan transformasi dengan one-hot-encoding seperti pada fitur kategori. Kita akan menggunakan teknik StandarScaler dari library Scikitlearn, StandardScaler melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi. StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0. Sekitar 68% dari nilai akan berada di antara -1 dan 1.

Untuk menghindari kebocoran informasi pada data uji, kita hanya akan menerapkan fitur standarisasi pada data latih. Kemudian, pada tahap evaluasi, kita akan melakukan standarisasi pada data uji.
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['carports', 'maid_bedrooms', 'maid_bathrooms']

scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.fit_transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Untuk mengecek nilai mean dan standar deviasi pada setelah proses standarisasi, jalankan kode berikut"""

X_train[numerical_features].describe().round(4)

"""Perhatikan tabel di atas, sekarang nilai mean = 0 dan standar deviasi = 1

  kita telah siap untuk dilatih menggunakan model machine learning

# 8. Model Development
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index = ["train_mse", "test_mse"],
                      columns = ["KNN", "RandomForest", "Boosting"])

"""## A. Model Development dengan K-Nearest Neighbor"""

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)
models.loc["train_mse","knn"] = mean_squared_error(y_pred = knn.predict(X_train), y_true = y_train)

"""## B. Model Development dengan Random Forest"""

RF = RandomForestRegressor(n_estimators = 50, max_depth = 16, random_state = 55, n_jobs = -1)
RF.fit(X_train, y_train)
models.loc["train_mse", "RandomForest"] = mean_squared_error(y_pred = RF.predict(X_train), y_true = y_train)

"""## C. Model Development dengan Boosting Algorithm"""

boosting = AdaBoostRegressor(learning_rate = 0.05, random_state = 55)
boosting.fit(X_train, y_train)
models.loc["train_mse", "Boosting"] = mean_squared_error(y_pred = boosting.predict(X_train), y_true = y_train)

"""# 9. Evaluasi Model"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata = 0 dan varians = 1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""Untuk evaluasi, kita akan menggunakan *Mean Squared Error* (MSE). MSE dirumuskan sebagai
$$MSE = \frac{\Sigma (y_i - \hat{y_i})^2}{n}$$
dengan
* $y_i$: Nilai y sesungguhnya
* $\hat{y_i}$: Nilai y prediksi
* $n$: Jumlah data

Terkadang, evaluasi juga dapat menggunakan *Root Mean Squared Error* (RMSE). Standarnya, RMSE adalah akar dari MSE sehingga dapat dirumuskan
$$RMSE = \sqrt{MSE} = \sqrt{\frac{\Sigma (y_i - \hat{y_i})^2}{n}}$$

Selain itu, ada yang disebut dengan *Mean Absolute Error* (MAE). MAE dirumuskan sebagai
$$MAE = \frac{\Sigma |y_i - \hat{y_i|}}{n}$$
"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns = ["train", "test"], index = ["KNN", "RF", "Boosting"])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {"KNN": knn, "RF": RF, "Boosting": boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, "train"] = mean_squared_error(y_true = y_train, y_pred = model.predict(X_train))
    mse.loc[name, "test"] = mean_squared_error(y_true = y_test, y_pred = model.predict(X_test))

# Panggil mse
mse

# MSE dari scratch

def MSE(y_true, y_test):
    sum_error = 0.0
    # Convert pandas Series to numpy arrays to use positional indexing
    y_true_values = y_true.values
    y_test_values = y_test

    # Ensure y_test_values is a NumPy array
    if not isinstance(y_test_values, np.ndarray):
        y_test_values = np.array(y_test_values)

    # loop over all values using the length of the shorter array
    for i in range(min(len(y_true_values), len(y_test_values))):
        # the error is the sum of (y_true - prediction)^2
        prediction_error =  y_true_values[i] - y_test_values[i]
        sum_error += (prediction_error ** 2)
    # now normalize using the length of the shorter array
    mean_error = sum_error / float(min(len(y_true_values), len(y_test_values)))
    return mean_error

mse_2 = pd.DataFrame(columns = ["train", "test"], index = ["KNN", "RF", "Boosting"])

for name, model in model_dict.items():
    mse_2.loc[name, "train"] = MSE(y_train, model.predict(X_train))
    mse_2.loc[name, "test"] = MSE(y_test, model.predict(X_test))

mse_2

# RMSE dari scratch

def RMSE(y_true, y_test):
    sum_error = 0.0
    # Convert pandas Series to numpy arrays to use positional indexing
    y_true_values = y_true.values
    y_test_values = y_test

    # Ensure y_test_values is a NumPy array
    if not isinstance(y_test_values, np.ndarray):
        y_test_values = np.array(y_test_values)

    # loop over all values using the length of the shorter array
    for i in range(min(len(y_true_values), len(y_test_values))):
        # the error is the sum of (y_true - prediction)^2
        prediction_error =  y_true_values[i] - y_test_values[i]
        sum_error += (prediction_error ** 2)
    # now normalize using the length of the shorter array
    mean_error = np.sqrt(sum_error / float(min(len(y_true_values), len(y_test_values))))
    return mean_error

rmse = pd.DataFrame(columns = ["train", "test"], index = ["KNN", "RF", "Boosting"])

for name, model in model_dict.items():
    rmse.loc[name, "train"] = RMSE(y_train, model.predict(X_train))
    rmse.loc[name, "test"] = RMSE(y_test, model.predict(X_test))

rmse

# MAE dari scratch

def MAE(y_true, y_test):
    sum_error = 0.0
    # Convert pandas Series to numpy arrays to use positional indexing
    y_true_values = y_true.values
    y_test_values = y_test

    # Ensure y_test_values is a NumPy array
    if not isinstance(y_test_values, np.ndarray):
        y_test_values = np.array(y_test_values)

    # loop over all values using the length of the shorter array
    for i in range(min(len(y_true_values), len(y_test_values))):
        # the error is the sum of (y_true - prediction)^2
        prediction_error =  y_true_values[i] - y_test_values[i]
        sum_error += np.abs(prediction_error)
    # Now normalize using the length of the shorter array
    mean_error = sum_error / float(min(len(y_true_values), len(y_test_values)))
    return mean_error

mae = pd.DataFrame(columns = ["train", "test"], index = ["KNN", "RF", "Boosting"])

for name, model in model_dict.items():
    mae.loc[name, "train"] = MAE(y_train, model.predict(X_train))
    mae.loc[name, "test"] = MAE(y_test, model.predict(X_test))

mae

fig, ax = plt.subplots()
mse.sort_values(by = "test", ascending = False).plot(kind = "barh", ax = ax, zorder = 3)
ax.grid(zorder = 0)

"""

*   RF (Random Forest): MSE untuk data pelatihan lebih rendah daripada data pengujian, yang menunjukkan bahwa model Random Forest memiliki performa yang baik pada data pelatihan, namun kinerjanya sedikit menurun pada data pengujian. Perbedaan ini bisa mengindikasikan adanya overfitting, tetapi tidak terlalu signifikan.

*   KNN (K-Nearest Neighbors): Model KNN memiliki MSE yang lebih rendah pada data pelatihan dibandingkan dengan data pengujian. Perbedaan yang signifikan antara kedua nilai MSE ini menunjukkan bahwa model ini mungkin mengalami overfitting, di mana ia cocok dengan data pelatihan tetapi kurang akurat saat diaplikasikan pada data pengujian.

*   Boosting: Model Boosting memiliki MSE yang cukup besar baik pada data pelatihan maupun pengujian. Besarnya MSE pada data pelatihan menunjukkan bahwa model ini mungkin mengalami kesulitan dalam menangkap pola pada data, atau terdapat underfitting, di mana model terlalu sederhana untuk menangkap kompleksitas data.

Secara keseluruhan, model yang lebih ideal adalah yang memiliki MSE rendah baik pada data pelatihan maupun pengujian. Dari hasil ini, kita dapat melihat bahwa RF memiliki performa yang lebih baik dibandingkan KNN dan Boosting, namun masih ada kemungkinan model ini overfit"""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Terlihat bahwa prediksi Random Forest (RF) memberikan hasil yang paling mendekati dengan y_true (data test). Dimana nilai y_true adalah 960000000.0 sedangkan nilai prediksi dari Random Forest adalah 954022231.8."""